{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import Scaler, InvertibleMapper, StaticCovariatesTransformer\n",
    "from darts.dataprocessing.transformers.missing_values_filler import MissingValuesFiller\n",
    "from darts.metrics import rmsle\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from darts.models import LinearRegressionModel, LightGBMModel, XGBModel, CatBoostModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from darts.models.filtering.moving_average_filter import MovingAverageFilter\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>N Batalla de Pichincha</th>\n",
       "      <th>N Carnaval</th>\n",
       "      <th>N Cyber Monday</th>\n",
       "      <th>N Dia de Difuntos</th>\n",
       "      <th>N Dia de la Madre</th>\n",
       "      <th>N Dia del Trabajo</th>\n",
       "      <th>N Futbol</th>\n",
       "      <th>N Independencia de Cuenca</th>\n",
       "      <th>N Independencia de Guayaquil</th>\n",
       "      <th>N Navidad</th>\n",
       "      <th>N Primer dia del ano</th>\n",
       "      <th>N Terremoto Manabi</th>\n",
       "      <th>N Viernes Santo</th>\n",
       "      <th>oil_price</th>\n",
       "      <th>transactions</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>year</th>\n",
       "      <th>is_wknd</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>date_index</th>\n",
       "      <th>season</th>\n",
       "      <th>workday</th>\n",
       "      <th>wageday</th>\n",
       "      <th>day_to_nearest_holiday</th>\n",
       "      <th>day_from_nearest_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_nbr      family  sales  onpromotion   city      state  \\\n",
       "0 2013-01-01          1  AUTOMOTIVE    0.0          0.0  Quito  Pichincha   \n",
       "1 2013-01-01          1   BABY CARE    0.0          0.0  Quito  Pichincha   \n",
       "2 2013-01-01          1      BEAUTY    0.0          0.0  Quito  Pichincha   \n",
       "3 2013-01-01          1   BEVERAGES    0.0          0.0  Quito  Pichincha   \n",
       "4 2013-01-01          1       BOOKS    0.0          0.0  Quito  Pichincha   \n",
       "\n",
       "  type  cluster  N Batalla de Pichincha  N Carnaval  N Cyber Monday  \\\n",
       "0    D       13                       0           0               0   \n",
       "1    D       13                       0           0               0   \n",
       "2    D       13                       0           0               0   \n",
       "3    D       13                       0           0               0   \n",
       "4    D       13                       0           0               0   \n",
       "\n",
       "   N Dia de Difuntos  N Dia de la Madre  N Dia del Trabajo  N Futbol  \\\n",
       "0                  0                  0                  0         0   \n",
       "1                  0                  0                  0         0   \n",
       "2                  0                  0                  0         0   \n",
       "3                  0                  0                  0         0   \n",
       "4                  0                  0                  0         0   \n",
       "\n",
       "   N Independencia de Cuenca  N Independencia de Guayaquil  N Navidad  \\\n",
       "0                          0                             0          0   \n",
       "1                          0                             0          0   \n",
       "2                          0                             0          0   \n",
       "3                          0                             0          0   \n",
       "4                          0                             0          0   \n",
       "\n",
       "   N Primer dia del ano  N Terremoto Manabi  N Viernes Santo  oil_price  \\\n",
       "0                     1                   0                0      93.14   \n",
       "1                     1                   0                0      93.14   \n",
       "2                     1                   0                0      93.14   \n",
       "3                     1                   0                0      93.14   \n",
       "4                     1                   0                0      93.14   \n",
       "\n",
       "   transactions  month  day_of_month  day_of_year  week_of_month  \\\n",
       "0           0.0      1             1            1              1   \n",
       "1           0.0      1             1            1              1   \n",
       "2           0.0      1             1            1              1   \n",
       "3           0.0      1             1            1              1   \n",
       "4           0.0      1             1            1              1   \n",
       "\n",
       "   week_of_year  day_of_week  year  is_wknd  quarter  is_month_start  \\\n",
       "0             1            2  2013        0        1               1   \n",
       "1             1            2  2013        0        1               1   \n",
       "2             1            2  2013        0        1               1   \n",
       "3             1            2  2013        0        1               1   \n",
       "4             1            2  2013        0        1               1   \n",
       "\n",
       "   is_month_end  is_quarter_start  is_quarter_end  is_year_start  is_year_end  \\\n",
       "0             0                 1               0              1            0   \n",
       "1             0                 1               0              1            0   \n",
       "2             0                 1               0              1            0   \n",
       "3             0                 1               0              1            0   \n",
       "4             0                 1               0              1            0   \n",
       "\n",
       "   date_index  season  workday  wageday  day_to_nearest_holiday  \\\n",
       "0           0       0        0        0                       0   \n",
       "1           0       0        0        0                       0   \n",
       "2           0       0        0        0                       0   \n",
       "3           0       0        0        0                       0   \n",
       "4           0       0        0        0                       0   \n",
       "\n",
       "   day_from_nearest_holiday  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'originalni_datasetovi\\train_test_final.csv', parse_dates=['date'])\n",
    "data.drop(columns=['work_day', 'lag_16_sales', 'lag_17_sales', 'lag_18_sales', 'lag_19_sales', 'lag_20_sales', 'lag_30_sales', 'lag_365_sales', 'lag_730_sales', 'lag_1_oil', 'lag_2_oil', 'lag_3_oil', 'lag_4_oil'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(static_covs_transform=False, log_transform=False):\n",
    "    '''\n",
    "    Function used for preprocessing time series data, fills missing values using interpolation. \\n\n",
    "    Optional keyword arguments:\n",
    "        static_covs_transform:  one hot encodes static covariates; defaults to False.\n",
    "        log_transform:  log transforms data; defaults to False.\n",
    "    '''\n",
    "    # contains all steps of our pipeline\n",
    "    lst = []\n",
    "    \n",
    "    # fills missing values, uses interpolation\n",
    "    # n_jobs = -1 -> allows for parallel processing using all available CPU cores\n",
    "    filler = MissingValuesFiller(n_jobs=-1)\n",
    "    lst.append(filler)\n",
    "    \n",
    "    # one hot encoding static covariates\n",
    "    if static_covs_transform:\n",
    "        static_covs_transformer = StaticCovariatesTransformer(\n",
    "            transformer_cat=OneHotEncoder(), \n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        lst.append(static_covs_transformer)\n",
    "\n",
    "    # perform log transformation on sales\n",
    "    if log_transform:\n",
    "        log_transformer = InvertibleMapper(\n",
    "            fn=np.log1p,\n",
    "            inverse_fn=np.expm1,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        lst.append(log_transformer)\n",
    "\n",
    "    # rescale time series\n",
    "    scaler = Scaler()\n",
    "    lst.append(scaler)\n",
    "\n",
    "    pipeline = Pipeline(lst)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_series(data, static_cols, log_transform=True):\n",
    "    '''\n",
    "    Extract and preprocess time series data for different product families. It applies transformations such as filling missing values, \n",
    "    log transformation, and scaling, and organizes the transformed data into dictionaries for further use.\\n\n",
    "    Parameters:\n",
    "        data: DataFrame from which target series will be extracted.\n",
    "        static_cols:  A string or list of strings representing static variable columns from the DataFrame that should be appended as static covariates to the resulting TimeSeries groups.\n",
    "    Optional keyword arguments:\n",
    "        log_transform:  log transforms data; defaults to True.  \n",
    "    '''    \n",
    "    target_dict = {} # key is family, value is array of time series of sales by stores\n",
    "    pipe_dict = {} # key is family, value is pipeline\n",
    "    id_dict = {} # key is family, value is pair of store and family\n",
    "\n",
    "    for fam in tqdm_notebook(data.family.unique(), desc=\"Extracting target series\"):\n",
    "        # using train and splitting by family\n",
    "        df = data[(data.date.le('2017-08-15')) & (data.family.eq(fam))] \n",
    "        \n",
    "        # initialize transformation pipeline for target series\n",
    "        pipe = get_pipeline(static_covs_transform=True, log_transform=log_transform)\n",
    "        \n",
    "        # The TimeSeries.from_group_dataframe method is used to create a list of time series objects from the dataframe, \n",
    "        # grouped by store_nbr and including static covariates specified in static_cols\n",
    "        target = TimeSeries.from_group_dataframe(\n",
    "            df=df,\n",
    "            time_col=\"date\",\n",
    "            value_cols=\"sales\",\n",
    "            group_cols=\"store_nbr\",\n",
    "            static_cols=static_cols,\n",
    "        )\n",
    "        \n",
    "        # record identity of each target series\n",
    "        target_id = [{\"store_nbr\": t.static_covariates.store_nbr, \"family\": fam} # pair family store\n",
    "                     for t in target]\n",
    "        id_dict[fam] = target_id\n",
    "\n",
    "        # apply transformations\n",
    "        target = pipe.fit_transform(target)\n",
    "        target_dict[fam] = [t.astype(np.float32) for t in target]\n",
    "\n",
    "        pipe_dict[fam] = pipe[2:]  # without MissingValuesFiller and OHEnc\n",
    "        \n",
    "    return target_dict, pipe_dict, id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012965917587280273,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting target series",
       "rate": null,
       "total": 33,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb386e189143447ca830287bb90cc663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting target series:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "static_cols = [\"city\", \"state\", \"type\", \"cluster\"]\n",
    "target_dict, pipe_dict, id_dict = get_target_series(data, static_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariates(data, past_cols, future_cols, future_ma_cols=None, future_window_sizes=[7, 28]):\n",
    "    '''\n",
    "    Extract and preprocess time series data for different product families. It applies transformations such as filling missing values, \n",
    "    log transformation, and scaling, and organizes the transformed data into dictionaries for further use.\\n\n",
    "    Parameters:\n",
    "        data: DataFrame from which future and past covariates will be extracted.\n",
    "        past_cols:  covariates known only into the past.\n",
    "        future_cols:  covariates known into the future.\n",
    "    Optional parameters:\n",
    "        future_ma_cols: columns that moving average will be computed on.\n",
    "        future_window_sizes: window size for moving average.\n",
    "    '''\n",
    "    past_dict = {} #key is family, value is array of time series of transactions by stores\n",
    "    future_dict = {} #key is family, value is array of array of time series of future covariate by stores for all future covariates\n",
    "    \n",
    "    # initialize transformation pipeline for covariates\n",
    "    covs_pipe = get_pipeline()\n",
    "\n",
    "    for fam in tqdm_notebook(data.family.unique(), desc=\"Extracting covariates\"):\n",
    "        # filter data for each model\n",
    "        df = data[data.family.eq(fam)]\n",
    "        \n",
    "        # extract past covariates\n",
    "        past_covs = TimeSeries.from_group_dataframe(\n",
    "            df=df[df.date.le('2017-08-15')],\n",
    "            time_col=\"date\",\n",
    "            value_cols=past_cols,\n",
    "            group_cols=\"store_nbr\",\n",
    "        )\n",
    "        past_covs = [p.with_static_covariates(None) for p in past_covs]\n",
    "        past_covs = covs_pipe.fit_transform(past_covs)\n",
    "        \n",
    "        past_dict[fam] = [p.astype(np.float32) for p in past_covs]\n",
    "\n",
    "        # extract future covariates\n",
    "        future_covs = TimeSeries.from_group_dataframe(\n",
    "            df=df,\n",
    "            time_col=\"date\",\n",
    "            value_cols=future_cols,\n",
    "            group_cols=\"store_nbr\",\n",
    "        )\n",
    "        future_covs = [f.with_static_covariates(None) for f in future_covs]\n",
    "        future_covs = covs_pipe.fit_transform(future_covs)\n",
    "        \n",
    "        if future_ma_cols is not None:\n",
    "            for size in future_window_sizes:\n",
    "                ma_filter = MovingAverageFilter(window=size)\n",
    "                old_names = [f\"rolling_mean_{size}_{col}\" for col in future_ma_cols]\n",
    "                new_names = [f\"{col}_ma{size}\" for col in future_ma_cols]\n",
    "                future_ma_covs = [\n",
    "                    ma_filter.filter(f[future_ma_cols]).with_columns_renamed(old_names, new_names) \n",
    "                    for f in future_covs\n",
    "                ]\n",
    "                future_covs = [f.stack(f_ma) for f, f_ma in zip(future_covs, future_ma_covs)]\n",
    "        \n",
    "        future_dict[fam] = [f.astype(np.float32) for f in future_covs]\n",
    "            \n",
    "    return past_dict, future_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008122682571411133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting covariates",
       "rate": null,
       "total": 33,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2755317c4a1a401ca942ab2a544dd5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting covariates:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# past covariates\n",
    "past_cols = [\"transactions\"]\n",
    "\n",
    "# future covariates\n",
    "future_cols = [\n",
    "    'N Dia de Difuntos', 'N Dia de la Madre', 'N Dia del Trabajo', 'N Futbol', 'N Navidad', 'N Primer dia del ano', 'N Terremoto Manabi', \"oil_price\", \"month\", \"day_of_year\", \"week_of_year\", \"day_of_week\", \"year\", \"date_index\", \"workday\", \"onpromotion\"]\n",
    "\n",
    "# columns for moving average\n",
    "future_ma_cols = [\"oil_price\", \"onpromotion\"]\n",
    "\n",
    "past_dict, future_dict = get_covariates(data, past_cols, future_cols, future_ma_cols=future_ma_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_CONFIG = {\n",
    "    \"target_dict\": target_dict,\n",
    "    \"pipe_dict\": pipe_dict,\n",
    "    \"id_dict\": id_dict,\n",
    "    \"past_dict\": past_dict,\n",
    "    \"future_dict\": future_dict,\n",
    "    \n",
    "    # time series cross-validation using a rolling forecasting origin\n",
    "    \"forecast_horizon\": 16, # the length of the validation set\n",
    "    \"folds\": 1, # the number of training sets (setting to 1 means the standard train-validation split)\n",
    "    \n",
    "    # the number of previous days to check for zero sales; if all are zero, generate zero forecasts\n",
    "    \"zero_fc_window\": 15,\n",
    "    \n",
    "    \"static_covs\": \"keep_all\",\n",
    "    \"past_covs\": \"keep_all\",\n",
    "    \"future_covs\": \"keep_all\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Trainer:\n",
    "    target_dict: dict \n",
    "    pipe_dict: dict \n",
    "    id_dict: dict \n",
    "    past_dict: dict \n",
    "    future_dict: dict \n",
    "    forecast_horizon: int \n",
    "    folds: int\n",
    "    zero_fc_window: int \n",
    "    static_covs: str\n",
    "    past_covs: str\n",
    "    future_covs: str\n",
    "        \n",
    "    def clip(self, array):\n",
    "        '''\n",
    "        Changes negative values of an array to zeroes.\n",
    "        '''\n",
    "        return np.clip(array, a_min=0., a_max=None)\n",
    "    \n",
    "    def train_valid_split(self, target, length):\n",
    "        # length is (self.folds - j) * self.forecast_horizon \n",
    "        train = [timeseries[:-length] for timeseries in target]\n",
    "        valid_end_idx = -length + self.forecast_horizon\n",
    "        \n",
    "        if valid_end_idx >= 0:\n",
    "            valid_end_idx = None\n",
    "        \n",
    "        valid = [t[-length:valid_end_idx] for t in target]\n",
    "        return train, valid\n",
    "    \n",
    "    def get_models(self, model_names, model_configs):\n",
    "        models = {\n",
    "            \"lr\": LinearRegressionModel,\n",
    "            \"lgbm\": LightGBMModel,\n",
    "            \"cat\": CatBoostModel,\n",
    "            \"xgb\": XGBModel,\n",
    "        }\n",
    "        assert isinstance(model_names, list) and isinstance(model_configs, list),\\\n",
    "        \"Both the model names and model configurations must be specified in lists.\"\n",
    "        assert all(name in models for name in model_names),\\\n",
    "        f\"Model names '{model_names}' not recognized.\"\n",
    "        assert len(model_names) == len(model_configs),\\\n",
    "        \"The number of model names and the number of model configurations do not match.\"\n",
    "        \n",
    "        if \"xgb\" in model_names:\n",
    "            xgb_idx = np.where(np.array(model_names)==\"xgb\")[0]\n",
    "            for idx in xgb_idx:\n",
    "                # change to histogram-based method for XGBoost to get faster training time\n",
    "                model_configs[idx] = {\"tree_method\": \"hist\", **model_configs[idx]}\n",
    "        \n",
    "        return [models[name](**model_configs[j]) for j, name in enumerate(model_names)]\n",
    "    \n",
    "    def generate_forecasts(self, models, train, pipe, past_covs, future_covs, drop_before):\n",
    "        if drop_before is not None: \n",
    "            date = pd.Timestamp(drop_before) - pd.Timedelta(days=1) \n",
    "            # train without specifed dates\n",
    "            train = [t.drop_before(date) for t in train]\n",
    "             \n",
    "        # inputs for a model\n",
    "        inputs = {\n",
    "            \"series\": train,\n",
    "            \"past_covariates\": past_covs,\n",
    "            \"future_covariates\": future_covs,\n",
    "        }\n",
    "\n",
    "        # generates validation dates and all zero values\n",
    "        zero_pred = pd.DataFrame({ \n",
    "            \"date\": pd.date_range(train[0].end_time(), periods=self.forecast_horizon+1)[1:],\n",
    "            \"sales\": np.zeros(self.forecast_horizon),\n",
    "        })\n",
    "        \n",
    "        # transforming that df to time series\n",
    "        zero_pred = TimeSeries.from_dataframe( \n",
    "            df=zero_pred,\n",
    "            time_col=\"date\",\n",
    "            value_cols=\"sales\",\n",
    "        )\n",
    "        \n",
    "        pred_list = []\n",
    "        ens_pred = [0 for _ in range(len(train))] # zero for every store \n",
    "        \n",
    "        for m in models:\n",
    "            # fit training data to model\n",
    "            m.fit(**inputs)\n",
    "\n",
    "            # generate forecasts\n",
    "            pred = m.predict(n=self.forecast_horizon, **inputs, show_warnings=False)\n",
    "            # apply inverse transformations\n",
    "            pred = pipe.inverse_transform(pred)\n",
    "\n",
    "            for j in range(len(train)):\n",
    "                # if there is all zeros in j time series in the last specifed period of time predict zeros\n",
    "                if train[j][-self.zero_fc_window:].values().sum() == 0:\n",
    "                    pred[j] = zero_pred\n",
    "            \n",
    "            # clip negative forecasts to 0s\n",
    "            pred = [p.map(self.clip) for p in pred]\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "            # ensemble averaging\n",
    "            for j in range(len(ens_pred)): # 54\n",
    "                ens_pred[j] += pred[j] / len(models) \n",
    "\n",
    "        return pred_list, ens_pred\n",
    "    \n",
    "    def metric(self, valid, pred):\n",
    "        valid_df = pd.concat([ts.pd_dataframe() for ts in valid], axis=1)\n",
    "        pred_df = pd.concat([ts.pd_dataframe() for ts in pred], axis=1)\n",
    "\n",
    "        # calculate RMSLE for each pair of valid and predicted values\n",
    "        rmsle_values = [mean_squared_log_error(valid_df[col], pred_df[col],squared=False) for col in valid_df.columns]\n",
    "\n",
    "        # calculate the mean of RMSLE values of all series of that family\n",
    "        mean_rmsle = np.mean(rmsle_values)\n",
    "\n",
    "        return mean_rmsle\n",
    "    \n",
    "    def validate(self, model_names, model_configs, drop_before=None):\n",
    "        # helper value to align printed text below\n",
    "        longest_len = len(max(self.target_dict.keys(), key=len)) #33 kao broj prodavnica\n",
    "        \n",
    "        # store metric values for each model\n",
    "        model_metrics_history = []\n",
    "        ens_metric_history = []\n",
    "        \n",
    "        for fam in tqdm_notebook(self.target_dict, desc=\"Performing validation\"):\n",
    "            target = self.target_dict[fam]\n",
    "            pipe = self.pipe_dict[fam]\n",
    "            past_covs = self.past_dict[fam]\n",
    "            future_covs = self.future_dict[fam]\n",
    "            \n",
    "            # record average metric value over all folds\n",
    "            model_metrics = []\n",
    "            ens_metric = 0\n",
    "            \n",
    "            for j in range(self.folds):    # folds = 1\n",
    "                # perform train-validation split and apply transformations\n",
    "                length = (self.folds - j) * self.forecast_horizon # 16\n",
    "                train, valid = self.train_valid_split(target, length) \n",
    "                valid = pipe.inverse_transform(valid) \n",
    "\n",
    "                # generate forecasts and compute metric\n",
    "                models = self.get_models(model_names, model_configs)\n",
    "                pred_list, ens_pred = self.generate_forecasts(models, train, pipe, past_covs, future_covs, drop_before) \n",
    "                metric_list = [self.metric(valid, pred) / self.folds for pred in pred_list]\n",
    "                model_metrics.append(metric_list)\n",
    "                if len(models) > 1:\n",
    "                    ens_metric_fold = self.metric(valid, ens_pred) / self.folds\n",
    "                    ens_metric += ens_metric_fold\n",
    "                \n",
    "            # store final metric value for each model\n",
    "            model_metrics = np.sum(model_metrics, axis=0)\n",
    "            model_metrics_history.append(model_metrics)\n",
    "            ens_metric_history.append(ens_metric)\n",
    "            \n",
    "            # print metric value for each family\n",
    "            print(\n",
    "                fam,\n",
    "                \" \" * (longest_len - len(fam)),\n",
    "                \" | \",\n",
    "                \" - \".join([f\"{model}: {metric:.5f}\" for model, metric in zip(model_names, model_metrics)]),\n",
    "                f\" - ens: {ens_metric:.5f}\" if len(models) > 1 else \"\",\n",
    "                sep=\"\",\n",
    "            )\n",
    "            \n",
    "        # print overall metric value\n",
    "        print(\n",
    "            \"Average RMSLE | \"\n",
    "            + \" - \".join([f\"{model}: {metric:.5f}\" \n",
    "                          for model, metric in zip(model_names, np.mean(model_metrics_history, axis=0))])\n",
    "            + (f\" - ens: {np.mean(ens_metric_history):.5f}\" if len(models) > 1 else \"\"),\n",
    "        )\n",
    "        \n",
    "    def ensemble_predict(self, model_names, model_configs, drop_before=None):        \n",
    "        forecasts = []\n",
    "        for fam in tqdm_notebook(self.target_dict.keys(), desc=\"Generating forecasts\"):\n",
    "            target = self.target_dict[fam]\n",
    "            pipe = self.pipe_dict[fam]\n",
    "            target_id = self.id_dict[fam]\n",
    "            past_covs = self.past_dict[fam]\n",
    "            future_covs = self.future_dict[fam]\n",
    "            \n",
    "            models = self.get_models(model_names, model_configs)\n",
    "            pred_list, ens_pred = self.generate_forecasts(models, target, pipe, past_covs, future_covs, drop_before)\n",
    "            ens_pred = [p.pd_dataframe().assign(store_nbr=i['store_nbr']['sales'], family=i['family']) for p, i in zip(ens_pred, target_id)]\n",
    "            ens_pred = pd.concat(ens_pred, axis=0)\n",
    "            forecasts.append(ens_pred)\n",
    "            \n",
    "        # combine all forecasts into one dataframe\n",
    "        forecasts = pd.concat(forecasts, axis=0)\n",
    "        forecasts = forecasts.rename_axis(None, axis=1).reset_index(names=\"date\")\n",
    "        \n",
    "        return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(**TRAINER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"random_state\": 0,\n",
    "    \n",
    "    # the number of lag values of the target series\n",
    "    \"lags\": 63,\n",
    "    \n",
    "    # the number of lag values of the past covariates\n",
    "    \"lags_past_covariates\": list(range(-16, -23, -1)),\n",
    "    \n",
    "    # the number of (past, future-1) lag values of the future covariates\n",
    "    \"lags_future_covariates\": (14, 1),\n",
    "    \n",
    "    # the number of days ahead that the model is forecasting given today's input data\n",
    "    \"output_chunk_length\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005984783172607422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Performing validation",
       "rate": null,
       "total": 33,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda28f933dff491f9db9b3e55382ab02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Performing validation:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTOMOTIVE                 | lr: 0.49327\n",
      "BABY CARE                  | lr: 0.18225\n",
      "BEAUTY                     | lr: 0.48382\n",
      "BEVERAGES                  | lr: 0.20918\n",
      "BOOKS                      | lr: 0.03108\n",
      "BREAD/BAKERY               | lr: 0.19334\n",
      "CELEBRATION                | lr: 0.53190\n",
      "CLEANING                   | lr: 0.30859\n",
      "DAIRY                      | lr: 0.17460\n",
      "DELI                       | lr: 0.18786\n",
      "EGGS                       | lr: 0.27028\n",
      "FROZEN FOODS               | lr: 0.27630\n",
      "GROCERY I                  | lr: 0.16795\n",
      "GROCERY II                 | lr: 0.53997\n",
      "HARDWARE                   | lr: 0.51593\n",
      "HOME AND KITCHEN I         | lr: 0.49408\n",
      "HOME AND KITCHEN II        | lr: 0.45205\n",
      "HOME APPLIANCES            | lr: 0.28248\n",
      "HOME CARE                  | lr: 0.21183\n",
      "LADIESWEAR                 | lr: 0.50623\n",
      "LAWN AND GARDEN            | lr: 0.44412\n",
      "LINGERIE                   | lr: 0.61564\n",
      "LIQUOR,WINE,BEER           | lr: 0.48699\n",
      "MAGAZINES                  | lr: 0.49007\n",
      "MEATS                      | lr: 0.20501\n",
      "PERSONAL CARE              | lr: 0.24419\n",
      "PET SUPPLIES               | lr: 0.45593\n",
      "PLAYERS AND ELECTRONICS    | lr: 0.44647\n",
      "POULTRY                    | lr: 0.21045\n",
      "PREPARED FOODS             | lr: 0.26911\n",
      "PRODUCE                    | lr: 0.42584\n",
      "SCHOOL AND OFFICE SUPPLIES | lr: 0.69874\n",
      "SEAFOOD                    | lr: 0.46194\n",
      "Average RMSLE | lr: 0.36265\n"
     ]
    }
   ],
   "source": [
    "trainer.validate([\"lr\"], [BASE_CONFIG], drop_before=\"2015-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009974002838134766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Performing validation",
       "rate": null,
       "total": 33,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b97b401f03345029e9a91736ffb8933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Performing validation:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTOMOTIVE                 | lgbm: 0.50570 - lgbm: 0.50604 - lgbm: 0.51086 - lgbm: 0.51247 - ens: 0.50254\n",
      "BABY CARE                  | lgbm: 0.20649 - lgbm: 0.20381 - lgbm: 0.21096 - lgbm: 0.20666 - ens: 0.20542\n",
      "BEAUTY                     | lgbm: 0.46667 - lgbm: 0.50958 - lgbm: 0.47006 - lgbm: 0.47430 - ens: 0.47147\n",
      "BEVERAGES                  | lgbm: 0.24228 - lgbm: 0.25068 - lgbm: 0.23535 - lgbm: 0.24958 - ens: 0.23261\n",
      "BOOKS                      | lgbm: 0.05721 - lgbm: 0.05388 - lgbm: 0.05237 - lgbm: 0.04573 - ens: 0.05195\n",
      "BREAD/BAKERY               | lgbm: 0.18822 - lgbm: 0.19438 - lgbm: 0.17740 - lgbm: 0.16981 - ens: 0.17652\n",
      "CELEBRATION                | lgbm: 0.52838 - lgbm: 0.54156 - lgbm: 0.52413 - lgbm: 0.53648 - ens: 0.52407\n",
      "CLEANING                   | lgbm: 0.32624 - lgbm: 0.36302 - lgbm: 0.31425 - lgbm: 0.34321 - ens: 0.32493\n",
      "DAIRY                      | lgbm: 0.14960 - lgbm: 0.17488 - lgbm: 0.14309 - lgbm: 0.15696 - ens: 0.15003\n",
      "DELI                       | lgbm: 0.19366 - lgbm: 0.20621 - lgbm: 0.17956 - lgbm: 0.17911 - ens: 0.18262\n",
      "EGGS                       | lgbm: 0.27418 - lgbm: 0.28335 - lgbm: 0.26926 - lgbm: 0.27543 - ens: 0.26689\n",
      "FROZEN FOODS               | lgbm: 0.26708 - lgbm: 0.28267 - lgbm: 0.25508 - lgbm: 0.26369 - ens: 0.25783\n",
      "GROCERY I                  | lgbm: 0.18289 - lgbm: 0.20603 - lgbm: 0.16714 - lgbm: 0.16462 - ens: 0.16896\n",
      "GROCERY II                 | lgbm: 0.55135 - lgbm: 0.55352 - lgbm: 0.52618 - lgbm: 0.54288 - ens: 0.53212\n",
      "HARDWARE                   | lgbm: 0.52027 - lgbm: 0.53600 - lgbm: 0.51870 - lgbm: 0.51770 - ens: 0.51726\n",
      "HOME AND KITCHEN I         | lgbm: 0.49432 - lgbm: 0.51386 - lgbm: 0.50227 - lgbm: 0.49407 - ens: 0.49143\n",
      "HOME AND KITCHEN II        | lgbm: 0.43608 - lgbm: 0.45324 - lgbm: 0.43381 - lgbm: 0.44455 - ens: 0.43421\n",
      "HOME APPLIANCES            | lgbm: 0.31110 - lgbm: 0.30884 - lgbm: 0.33563 - lgbm: 0.30425 - ens: 0.31251\n",
      "HOME CARE                  | lgbm: 0.23040 - lgbm: 0.24936 - lgbm: 0.22020 - lgbm: 0.20998 - ens: 0.21684\n",
      "LADIESWEAR                 | lgbm: 0.42606 - lgbm: 0.43055 - lgbm: 0.43263 - lgbm: 0.42169 - ens: 0.41915\n",
      "LAWN AND GARDEN            | lgbm: 0.34333 - lgbm: 0.35895 - lgbm: 0.37059 - lgbm: 0.38444 - ens: 0.35702\n",
      "LINGERIE                   | lgbm: 0.63576 - lgbm: 0.63357 - lgbm: 0.63656 - lgbm: 0.63697 - ens: 0.62429\n",
      "LIQUOR,WINE,BEER           | lgbm: 0.41922 - lgbm: 0.45237 - lgbm: 0.42813 - lgbm: 0.42307 - ens: 0.41089\n",
      "MAGAZINES                  | lgbm: 0.50819 - lgbm: 0.51622 - lgbm: 0.50539 - lgbm: 0.52446 - ens: 0.50628\n",
      "MEATS                      | lgbm: 0.21438 - lgbm: 0.22073 - lgbm: 0.20471 - lgbm: 0.20336 - ens: 0.20475\n",
      "PERSONAL CARE              | lgbm: 0.23815 - lgbm: 0.28646 - lgbm: 0.22890 - lgbm: 0.22480 - ens: 0.22928\n",
      "PET SUPPLIES               | lgbm: 0.46279 - lgbm: 0.46420 - lgbm: 0.46139 - lgbm: 0.46031 - ens: 0.45529\n",
      "PLAYERS AND ELECTRONICS    | lgbm: 0.44900 - lgbm: 0.46447 - lgbm: 0.45310 - lgbm: 0.46619 - ens: 0.45026\n",
      "POULTRY                    | lgbm: 0.22201 - lgbm: 0.24433 - lgbm: 0.20682 - lgbm: 0.20995 - ens: 0.21300\n",
      "PREPARED FOODS             | lgbm: 0.26857 - lgbm: 0.28107 - lgbm: 0.26399 - lgbm: 0.25881 - ens: 0.26036\n",
      "PRODUCE                    | lgbm: 0.14619 - lgbm: 0.15304 - lgbm: 0.14366 - lgbm: 0.14198 - ens: 0.13986\n",
      "SCHOOL AND OFFICE SUPPLIES | lgbm: 0.55766 - lgbm: 0.55255 - lgbm: 0.55857 - lgbm: 0.56503 - ens: 0.54981\n",
      "SEAFOOD                    | lgbm: 0.48131 - lgbm: 0.47392 - lgbm: 0.47171 - lgbm: 0.48406 - ens: 0.46611\n",
      "Average RMSLE | lgbm: 0.34863 - lgbm: 0.36131 - lgbm: 0.34583 - lgbm: 0.34838 - ens: 0.34262\n"
     ]
    }
   ],
   "source": [
    "GBDT_CONFIG1 = {\n",
    "    **BASE_CONFIG,\n",
    "    \"verbose\":-1\n",
    "    # the additional hyperparameters to be specified\n",
    "#     \"n_estimators\": 100,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"max_depth\": 6,\n",
    "    \n",
    "}\n",
    "\n",
    "GBDT_CONFIG2 = GBDT_CONFIG1.copy()\n",
    "GBDT_CONFIG2[\"lags\"] = 7\n",
    "\n",
    "GBDT_CONFIG3 = GBDT_CONFIG1.copy()\n",
    "GBDT_CONFIG3[\"lags\"] = 365\n",
    "\n",
    "GBDT_CONFIG4 = GBDT_CONFIG1.copy()\n",
    "GBDT_CONFIG4[\"lags\"] = 730\n",
    "\n",
    "\n",
    "\n",
    "# 'lgbm' for LightGBM, 'xgb' for XGBoost, 'cat' for CatBoost\n",
    "ENS_MODELS = [\"lgbm\", \"lgbm\", \"lgbm\", \"lgbm\"]\n",
    "ENS_CONFIGS = [GBDT_CONFIG1, GBDT_CONFIG2, GBDT_CONFIG3, GBDT_CONFIG4]\n",
    "\n",
    "trainer.validate(\n",
    "    model_names=ENS_MODELS, \n",
    "    model_configs=ENS_CONFIGS,\n",
    "    drop_before=\"2015-01-01\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0069806575775146484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating forecasts",
       "rate": null,
       "total": 33,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e5b9da6f45494bb14764f0ad262c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating forecasts:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictions1 = trainer.ensemble_predict(\n",
    "#     model_names=ENS_MODELS, \n",
    "#     model_configs=ENS_CONFIGS,\n",
    "# )\n",
    "\n",
    "# generate forecasts for model trained on a subset of the data\n",
    "predictions2 = trainer.ensemble_predict(\n",
    "    model_names=ENS_MODELS, \n",
    "    model_configs=ENS_CONFIGS,\n",
    "    drop_before=\"2015-01-01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>3.293292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>2.968894</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>3.360378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>4.467464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>1.770783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     sales  store_nbr      family\n",
       "0 2017-08-16  3.293292        1.0  AUTOMOTIVE\n",
       "1 2017-08-17  2.968894        1.0  AUTOMOTIVE\n",
       "2 2017-08-18  3.360378        1.0  AUTOMOTIVE\n",
       "3 2017-08-19  4.467464        1.0  AUTOMOTIVE\n",
       "4 2017-08-20  1.770783        1.0  AUTOMOTIVE"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['sales_x', 'sales_y'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[248], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# compute the average of the ensemble models\u001b[39;00m\n\u001b[0;32m      2\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m predictions2\n\u001b[1;32m----> 3\u001b[0m final_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_predictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msales_x\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msales_y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m final_predictions\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_x\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_y\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5937\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['sales_x', 'sales_y'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# compute the average of the ensemble models\n",
    "final_predictions = predictions1.merge(\n",
    "    predictions2, on=[\"date\", 'store_nbr', \"family\"], how=\"left\",\n",
    ")\n",
    "final_predictions[\"sales\"] = final_predictions[[\"sales_x\", \"sales_y\"]].mean(axis=1)\n",
    "final_predictions = final_predictions.drop(columns=[\"sales_x\", \"sales_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=data[data['date']>= '2017-08-16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>N Batalla de Pichincha</th>\n",
       "      <th>N Carnaval</th>\n",
       "      <th>N Cyber Monday</th>\n",
       "      <th>N Dia de Difuntos</th>\n",
       "      <th>N Dia de la Madre</th>\n",
       "      <th>N Dia del Trabajo</th>\n",
       "      <th>N Futbol</th>\n",
       "      <th>N Independencia de Cuenca</th>\n",
       "      <th>N Independencia de Guayaquil</th>\n",
       "      <th>N Navidad</th>\n",
       "      <th>N Primer dia del ano</th>\n",
       "      <th>N Terremoto Manabi</th>\n",
       "      <th>N Viernes Santo</th>\n",
       "      <th>oil_price</th>\n",
       "      <th>transactions</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>year</th>\n",
       "      <th>is_wknd</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>date_index</th>\n",
       "      <th>season</th>\n",
       "      <th>workday</th>\n",
       "      <th>wageday</th>\n",
       "      <th>day_to_nearest_holiday</th>\n",
       "      <th>day_from_nearest_holiday</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3008016</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>228</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1688</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>3000888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008017</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>228</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1688</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>3000889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008018</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>228</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1688</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>3000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008019</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>228</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1688</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>3000891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008020</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>228</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1688</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>3000892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  store_nbr      family  sales  onpromotion   city  \\\n",
       "3008016 2017-08-16          1  AUTOMOTIVE    NaN          0.0  Quito   \n",
       "3008017 2017-08-16          1   BABY CARE    NaN          0.0  Quito   \n",
       "3008018 2017-08-16          1      BEAUTY    NaN          2.0  Quito   \n",
       "3008019 2017-08-16          1   BEVERAGES    NaN         20.0  Quito   \n",
       "3008020 2017-08-16          1       BOOKS    NaN          0.0  Quito   \n",
       "\n",
       "             state type  cluster  N Batalla de Pichincha  N Carnaval  \\\n",
       "3008016  Pichincha    D       13                       0           0   \n",
       "3008017  Pichincha    D       13                       0           0   \n",
       "3008018  Pichincha    D       13                       0           0   \n",
       "3008019  Pichincha    D       13                       0           0   \n",
       "3008020  Pichincha    D       13                       0           0   \n",
       "\n",
       "         N Cyber Monday  N Dia de Difuntos  N Dia de la Madre  \\\n",
       "3008016               0                  0                  0   \n",
       "3008017               0                  0                  0   \n",
       "3008018               0                  0                  0   \n",
       "3008019               0                  0                  0   \n",
       "3008020               0                  0                  0   \n",
       "\n",
       "         N Dia del Trabajo  N Futbol  N Independencia de Cuenca  \\\n",
       "3008016                  0         0                          0   \n",
       "3008017                  0         0                          0   \n",
       "3008018                  0         0                          0   \n",
       "3008019                  0         0                          0   \n",
       "3008020                  0         0                          0   \n",
       "\n",
       "         N Independencia de Guayaquil  N Navidad  N Primer dia del ano  \\\n",
       "3008016                             0          0                     0   \n",
       "3008017                             0          0                     0   \n",
       "3008018                             0          0                     0   \n",
       "3008019                             0          0                     0   \n",
       "3008020                             0          0                     0   \n",
       "\n",
       "         N Terremoto Manabi  N Viernes Santo  oil_price  transactions  month  \\\n",
       "3008016                   0                0       46.8           0.0      8   \n",
       "3008017                   0                0       46.8           0.0      8   \n",
       "3008018                   0                0       46.8           0.0      8   \n",
       "3008019                   0                0       46.8           0.0      8   \n",
       "3008020                   0                0       46.8           0.0      8   \n",
       "\n",
       "         day_of_month  day_of_year  week_of_month  week_of_year  day_of_week  \\\n",
       "3008016            16          228              3            33            3   \n",
       "3008017            16          228              3            33            3   \n",
       "3008018            16          228              3            33            3   \n",
       "3008019            16          228              3            33            3   \n",
       "3008020            16          228              3            33            3   \n",
       "\n",
       "         year  is_wknd  quarter  is_month_start  is_month_end  \\\n",
       "3008016  2017        0        3               0             0   \n",
       "3008017  2017        0        3               0             0   \n",
       "3008018  2017        0        3               0             0   \n",
       "3008019  2017        0        3               0             0   \n",
       "3008020  2017        0        3               0             0   \n",
       "\n",
       "         is_quarter_start  is_quarter_end  is_year_start  is_year_end  \\\n",
       "3008016                 0               0              0            0   \n",
       "3008017                 0               0              0            0   \n",
       "3008018                 0               0              0            0   \n",
       "3008019                 0               0              0            0   \n",
       "3008020                 0               0              0            0   \n",
       "\n",
       "         date_index  season  workday  wageday  day_to_nearest_holiday  \\\n",
       "3008016        1688       2        1        0                       5   \n",
       "3008017        1688       2        1        0                       5   \n",
       "3008018        1688       2        1        0                       5   \n",
       "3008019        1688       2        1        0                       5   \n",
       "3008020        1688       2        1        0                       5   \n",
       "\n",
       "         day_from_nearest_holiday       id  \n",
       "3008016                        54  3000888  \n",
       "3008017                        54  3000889  \n",
       "3008018                        54  3000890  \n",
       "3008019                        54  3000891  \n",
       "3008020                        54  3000892  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=test.copy()\n",
    "start_id = 3000888\n",
    "end_id = start_id + len(test)\n",
    "ids = range(start_id, end_id)\n",
    "\n",
    "test['id'] = ids\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000888</td>\n",
       "      <td>3.266740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000889</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000890</td>\n",
       "      <td>3.955883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000891</td>\n",
       "      <td>2218.753332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000892</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id        sales\n",
       "0  3000888     3.266740\n",
       "1  3000889     0.000000\n",
       "2  3000890     3.955883\n",
       "3  3000891  2218.753332\n",
       "4  3000892     0.000000"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = predictions2.copy()\n",
    "submission = test.merge(\n",
    "    final, on=[\"date\", \"store_nbr\", \"family\"], how=\"left\",\n",
    ")\n",
    "submission=submission.reset_index()\n",
    "submission=submission[['id','sales_y']]\n",
    "submission.rename(columns={'sales_y': 'sales'}, inplace=True)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
